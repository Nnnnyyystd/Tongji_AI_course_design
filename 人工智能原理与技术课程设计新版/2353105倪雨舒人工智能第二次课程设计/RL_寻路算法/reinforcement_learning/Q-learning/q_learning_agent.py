import matplotlib.pyplot as plt
import numpy as np
import random
from environment import Env
from collections import defaultdict
from visual import plot_rewards, plot_q_value_heatmap, visualize_path_from_statess,plot_training_statistics,visualize_path_from_q_table

class QLearningAgent:
    def __init__(self, actions):
        # actions = [0, 1, 2, 3]
        self.actions = actions
        self.learning_rate = 0.1
        self.discount_factor = 0.9
        #self.epsilon = 0.1
        self.epsilon =1  # åˆå§‹ Îµï¼ˆå¤§çš„è¯é¼“åŠ±æ¢ç´¢ï¼‰
        self.epsilon_min = 0.1  # æœ€å° Îµï¼ˆè®­ç»ƒåæœŸä»ä¿ç•™å°‘é‡æ¢ç´¢ï¼‰
        self.epsilon_decay = 0.9995  # è¡°å‡ç³»æ•°ï¼ˆæˆ–ç”¨çº¿æ€§è¡°å‡ä¹Ÿå¯ä»¥ï¼‰

        self.q_table = defaultdict(lambda: [0.0, 0.0, 0.0, 0.0])

    # é‡‡æ · <s, a, r, s'>
    def learn(self, state, action, reward, next_state):
        current_q = self.q_table[state][action]
        # è´å°”æ›¼æ–¹ç¨‹æ›´æ–°
        new_q = reward + self.discount_factor * max(self.q_table[next_state])
        self.q_table[state][action] += self.learning_rate * (new_q - current_q)

    # ä»Q-tableä¸­é€‰å–åŠ¨ä½œ
    def get_action(self, state):
        if np.random.rand() < self.epsilon:
            # è´ªå©ªç­–ç•¥éšæœºæ¢ç´¢åŠ¨ä½œ
            action = np.random.choice(self.actions)
        else:
            # ä»qè¡¨ä¸­é€‰æ‹©
            state_action = self.q_table[state]
            action = self.arg_max(state_action)
        return action
    @staticmethod
    def arg_max(state_action):
        max_index_list = []
        max_value = state_action[0]
        for index, value in enumerate(state_action):
            if value > max_value:
                # æ‰¾åˆ°æ–°çš„æœ€å¤§å€¼ï¼Œæ¸…ç©ºå¹¶è®°å½•æ–°ç´¢å¼•
                max_index_list.clear()
                max_value = value
                max_index_list.append(index)
            elif value == max_value:
                # ä¸å½“å‰æœ€å¤§å€¼ç›¸ç­‰ï¼Œä¹ŸåŠ å…¥å€™é€‰
                max_index_list.append(index)
        # ä»æ‰€æœ‰æœ€å¤§å€¼ä¸­éšæœºé€‰ä¸€ä¸ªï¼ˆé¿å…ç­–ç•¥å›ºå®šï¼‰
        return random.choice(max_index_list)


if __name__ == "__main__":
    cnt = 200  # è®¾ç½®é‡å¤è®­ç»ƒæ¬¡æ•°
    env = Env()  # åˆ›å»ºå¼ºåŒ–å­¦ä¹ ç¯å¢ƒï¼ˆå›¾å½¢ç•Œé¢è¿·å®«ï¼‰

    # ç”¨äºç»Ÿè®¡æ¯è½®è®­ç»ƒè¿‡ç¨‹çš„å…³é”®æŒ‡æ ‡
    all_total_episodes = []  # æ¯è½®è®­ç»ƒæ‰€éœ€çš„ episode æ€»æ•°
    all_first_success = []  # æ¯è½®é¦–æ¬¡æˆåŠŸåˆ°è¾¾ç»ˆç‚¹çš„ episode ç¼–å·
    all_shortest_found = []  # æ¯è½®é¦–æ¬¡æ‰¾åˆ°æœ€çŸ­è·¯å¾„çš„ episode ç¼–å·
    all_min_steps = []  # æ¯è½®çš„æœ€çŸ­è·¯å¾„æ­¥æ•°
    # åˆå§‹åŒ– Q-learning æ™ºèƒ½ä½“
    agent = QLearningAgent(actions=list(range(env.n_actions)))
    rewards = []  # ä¿å­˜æ¯ä¸ª episode çš„æ€»å¥–åŠ±
    min_success_steps = float('inf')  # æœ€çŸ­æˆåŠŸè·¯å¾„æ­¥æ•°åˆå§‹åŒ–ä¸ºæ— ç©·å¤§
    tolerance = 2  # å‰ªæå®¹å¿åº¦ï¼Œè¶…å‡ºå½“å‰æœ€çŸ­è·¯å¾„ + tolerance å³å‰ªæ
    pause_after_better_path = 50  # æ¯æ¬¡å‘ç°æ›´çŸ­è·¯å¾„åæš‚åœå‰ªæçš„è½®æ•°
    pruning_paused_until = -1  # å½“å‰å‰ªææš‚åœæˆªæ­¢è½®æ¬¡
    skip_count = 0  # è¿ç»­è¢«å‰ªæ‰çš„æ¬¡æ•°
    skip_count_threshold = 20  # è¿ç»­è¢«å‰ªæ‰è¾¾åˆ°è¯¥å€¼åï¼Œè®­ç»ƒç»ˆæ­¢
    max_episodes = 20000  # æ¯è½®è®­ç»ƒæœ€å¤šè¿­ä»£çš„ episode æ•°
    find_shortest_path_epi = 0  # è®°å½•ç¬¬ä¸€æ¬¡æ‰¾åˆ°æœ€çŸ­è·¯å¾„çš„ episode ç¼–å·
    first_success_episode = None  # é¦–æ¬¡æˆåŠŸåˆ°è¾¾ç»ˆç‚¹çš„ episode ç¼–å·
    best_path = []  # å­˜å‚¨å½“å‰æœ€çŸ­è·¯å¾„ï¼ˆçŠ¶æ€åºåˆ—ï¼‰

    # episode è®­ç»ƒä¸»å¾ªç¯
    for episode in range(max_episodes):
        state = env.reset()  # é‡ç½®ç¯å¢ƒï¼Œè·å¾—åˆå§‹çŠ¶æ€
        step_count = 0  # å½“å‰ episode æ­¥æ•°è®¡æ•°å™¨
        total_reward = 0  # å½“å‰ episode çš„æ€»å¥–åŠ±
        success = False  # æ˜¯å¦æˆåŠŸåˆ°è¾¾ç»ˆç‚¹
        episode_path = []  # å½“å‰ episode çš„çŠ¶æ€è·¯å¾„
        # åˆ¤æ–­æ˜¯å¦å¯ç”¨å‰ªæï¼ˆéœ€é¦–æ¬¡æˆåŠŸåˆ°è¾¾ç»ˆç‚¹ + å‰ªææœªæš‚åœï¼‰
        pruning_enabled = (
                first_success_episode is not None and
                episode >= pruning_paused_until
        )

        # å•ä¸ª episode çš„ step å¾ªç¯
        while True:
            env.render()  # å¯è§†åŒ–å½“å‰çŠ¶æ€
            action = agent.get_action(str(state))  # ä½¿ç”¨ Îµ-è´ªå©ªç­–ç•¥é€‰åŠ¨ä½œ
            episode_path.append(state)  # è®°å½•è·¯å¾„
            next_state, reward, done = env.step(action)  # æ‰§è¡ŒåŠ¨ä½œï¼Œè·å–åé¦ˆ
            step_count += 1
            total_reward += reward
            # âœ… å¯åŠ¨å‰ªæé€»è¾‘ï¼ˆæˆåŠŸä¹‹å + æœªæš‚åœï¼‰
            if pruning_enabled and reward > 0 and step_count > min_success_steps + tolerance:
                #early_terminated = True
                print(f"[{episode}] âš ï¸ å‰ªæï¼šè·¯å¾„ {step_count} æ­¥ > å½“å‰æœ€ä¼˜ {min_success_steps} æ­¥")
                skip_count += 1
                break  # å½“å‰è·¯å¾„æ˜æ˜¾æ¯”å·²æœ‰æœ€çŸ­è·¯å¾„å·® â†’ æå‰ç»ˆæ­¢
            # Q-learning å…¬å¼æ›´æ–° Q å€¼
            agent.learn(str(state), action, reward, str(next_state))
            state = next_state  # çŠ¶æ€æ›´æ–°
            if done:  # episode ç»“æŸï¼ˆåˆ°è¾¾ç»ˆç‚¹æˆ–æ’å¢™ï¼‰
                if reward > 0 :
                    success = True
                    # âœ… ç¬¬ä¸€æ¬¡æˆåŠŸåˆ°è¾¾ç»ˆç‚¹ï¼Œè®°å½• episode ç¼–å·
                    if first_success_episode is None:
                        first_success_episode = episode
                    # å¦‚æœæ‰¾åˆ°æ›´çŸ­è·¯å¾„ï¼Œæ›´æ–°æœ€çŸ­è·¯å¾„ä¿¡æ¯
                    if step_count < min_success_steps:
                        min_success_steps = step_count
                        skip_count = 0  # å‰ªæå‘½ä¸­æ¬¡æ•°æ¸…é›¶
                        pruning_paused_until = episode + pause_after_better_path
                        find_shortest_path_epi = episode
                        best_path = episode_path.copy()
                        print(f"[{episode}] ğŸ¯ å‘ç°æ›´çŸ­è·¯å¾„ï¼š{step_count} æ­¥ â†’ æš‚åœå‰ªæè‡³ Episode {pruning_paused_until}")
                break  # episode ç»“æŸ
        # åœ¨ç¯å¢ƒä¸Šæ‰“å° Q è¡¨å€¼ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰
        env.print_value_all(agent.q_table)
        rewards.append(total_reward)
        #  Îµ è¡°å‡ç­–ç•¥
        if agent.epsilon > agent.epsilon_min:
            agent.epsilon *= agent.epsilon_decay
        #  å‰ªæå‘½ä¸­æ¬¡æ•°è¶…è¿‡é˜ˆå€¼ â†’ åˆ¤å®šè®­ç»ƒæ”¶æ•›ï¼Œåœæ­¢
        if skip_count >= skip_count_threshold:
            print("ğŸ è®­ç»ƒå®Œæˆï¼æ¨æµ‹æœ€çŸ­è·¯å¾„ä¸ºï¼š", min_success_steps)
            print("è®­ç»ƒæ€»è½®æ¬¡:", episode)
            print("é¦–æ¬¡æˆåŠŸæŠµè¾¾ç»ˆç‚¹çš„è½®æ¬¡:", first_success_episode)
            print("é¦–æ¬¡æ‰¾åˆ°æœ€çŸ­è·¯å¾„çš„è½®æ¬¡:", find_shortest_path_epi)
            # è®°å½•ç»Ÿè®¡æ•°æ®
            all_total_episodes.append(episode)
            all_first_success.append(first_success_episode)
            all_shortest_found.append(find_shortest_path_epi)
            all_min_steps.append(min_success_steps)
            break
        # æ¯éš” 10 è½®æ‰“å°è®­ç»ƒçŠ¶æ€
        if episode % 10 == 0:
            print(f"[Episode {episode}] Steps: {step_count}, Reward: {total_reward}, Success: {success}")
    plot_q_value_heatmap(env, agent.q_table)
    plot_rewards(rewards)
    if 'best_path' in locals() and len(best_path) > 0:
        print("ğŸ¯ æœ€ç»ˆæœ€çŸ­è·¯å¾„æ­¥æ•°ï¼š", len(best_path))
        visualize_path_from_statess(env, best_path)
    else:
        print("âš ï¸ æœªè®°å½•åˆ°æˆåŠŸè·¯å¾„ï¼Œæ— æ³•å¯è§†åŒ–")

    env.mainloop()
#   while(cnt>0):


   # plot_training_statistics(all_total_episodes, all_first_success, all_shortest_found, all_min_steps)
















