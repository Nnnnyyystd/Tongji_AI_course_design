
import matplotlib.pyplot as plt
import numpy as np

def visualize_path_from_q_table(env, q_table, max_steps=100):
    """
    æ ¹æ® Q è¡¨ç­–ç•¥å¯è§†åŒ–è·¯å¾„ï¼ˆæŒ‰æœ€å¤§ Q å€¼ä¾æ¬¡å‰è¿›ï¼‰ï¼Œç»¿è‰²é«˜äº®è·¯å¾„ã€‚
    """
    # æ‰¾åˆ°èµ·ç‚¹ï¼ˆå€¼ä¸º 3ï¼‰
    start = None
    for row in range(env.HEIGHT):
        for col in range(env.WIDTH):
            if env.state_map[row][col] == 3:
                start = [row, col]
                break
        if start:
            break

    if not start:
        print("â— æœªæ‰¾åˆ°èµ·ç‚¹")
        return

    current_state = start
    visited = set()
    path = [current_state]
    steps = 0

    while steps < max_steps:
        steps += 1
        state_key = str(current_state)

        if tuple(current_state) in visited:
            print(f"ğŸ” å¾ªç¯è·¯å¾„ç»ˆæ­¢ at {current_state}")
            break
        visited.add(tuple(current_state))

        if state_key not in q_table:
            print(f"â— Q è¡¨ä¸­æ— çŠ¶æ€ {state_key}")
            break

        # é€‰å–å½“å‰çŠ¶æ€ä¸‹ Q å€¼æœ€å¤§çš„åŠ¨ä½œ
        q_values = q_table[state_key]
        best_action = np.argmax(q_values)

        # æ ¹æ®åŠ¨ä½œæ›´æ–°ä¸‹ä¸€çŠ¶æ€
        next_state = current_state.copy()
        if best_action == 0 and next_state[0] > 0:
            next_state[0] -= 1
        elif best_action == 1 and next_state[0] < env.HEIGHT - 1:
            next_state[0] += 1
        elif best_action == 2 and next_state[1] > 0:
            next_state[1] -= 1
        elif best_action == 3 and next_state[1] < env.WIDTH - 1:
            next_state[1] += 1
        else:
            print("âš ï¸ éæ³•åŠ¨ä½œæˆ–è¶Šç•Œç»ˆæ­¢")
            break

        path.append(next_state)

        # âœ… ç”»å‡ºç»¿è‰²é«˜äº®è·¯å¾„
        x1 = next_state[1] * env.UNIT + 3
        y1 = next_state[0] * env.UNIT + 3
        x2 = x1 + env.UNIT - 6
        y2 = y1 + env.UNIT - 6
        rect = env.canvas.create_rectangle(x1, y1, x2, y2, fill='lime', outline='black', width=1)
        env.canvas.tag_raise(rect)

        # âœ… åˆ°è¾¾ç»ˆç‚¹åˆ™åœæ­¢
        if env.state_map[next_state[0]][next_state[1]] == 2:
            print("ğŸ¯ åˆ°è¾¾ç»ˆç‚¹")
            break

        current_state = next_state

    env.update()
    print(f"âœ… å¯è§†åŒ–ç­–ç•¥è·¯å¾„å®Œæˆï¼Œæ€»æ­¥æ•°ï¼š{len(path)}")






def visualize_path_from_statess(env, path):
    visited = set()
    for state in path:
        if tuple(state) in visited:
            continue
        visited.add(tuple(state))

        x1 = state[1] * env.UNIT + 3
        y1 = state[0] * env.UNIT + 3
        x2 = x1 + env.UNIT - 6
        y2 = y1 + env.UNIT - 6
        rect = env.canvas.create_rectangle(x1, y1, x2, y2, fill='lime', outline='black', width=1)
        env.canvas.tag_raise(rect)

    env.update()
    print(f"âœ… æˆåŠŸå¯è§†åŒ–æœ€çŸ­è·¯å¾„ï¼Œå…± {len(path)} æ­¥")

def plot_q_value_heatmap(env, q_table):

        print("heatmap =")
        #print(np.round(heatmap, 2))
        heatmap = np.full((env.HEIGHT, env.WIDTH), np.nan)  # NaN è¡¨ç¤ºä¸å¯è¾¾åŒºåŸŸï¼ˆéšœç¢ï¼‰

        for row in range(env.HEIGHT):
            for col in range(env.WIDTH):
                state = [row, col]
                key = str(state)
                if env.state_map[row][col] == 1:
                    continue  # éšœç¢ç‰©
                if key in q_table:
                    max_q = max(q_table[key])
                    heatmap[row, col] = max_q

        plt.figure(figsize=(8, 6))
        plt.title("ğŸ”¥ Q-value Heatmap (Max per State)")
        plt.xlabel("Column")
        plt.ylabel("Row")
        plt.imshow(heatmap, cmap='YlOrRd', interpolation='nearest')
        plt.colorbar(label='Max Q Value')
        #plt.gca().invert_yaxis()  # ä¿æŒåæ ‡ç³»å’Œç¯å¢ƒä¸€è‡´ï¼ˆèµ·ç‚¹åœ¨ä¸Šï¼‰
        plt.grid(True, linestyle='--', alpha=0.3)
        plt.show()
    # âœ… å¯è§†åŒ– reward è¶‹åŠ¿
def plot_rewards(reward_list):
        plt.figure(figsize=(10, 5))
        plt.plot(reward_list, label='Total Reward')
        plt.xlabel('Episode')
        plt.ylabel('Reward')
        plt.title('Reward Trend Over Training')
        plt.grid(True)
        plt.legend()
        plt.show()
def plot_training_statistics(all_total_episodes,all_first_success,all_shortest_found,all_min_steps):
    runs = list(range(1, len(all_total_episodes) + 1))

    plt.figure(figsize=(12, 6))
    plt.plot(runs, all_total_episodes, label="sum_episode", marker='o')
    plt.plot(runs, all_first_success, label="first_success_episode", marker='^')
    plt.plot(runs, all_shortest_found, label="find_shortest_path_episode", marker='s')
    plt.xlabel("episode_number")
    plt.ylabel("Episode")
    plt.title("critical_issue_contrast")
    plt.grid(True)
    plt.legend()
    plt.tight_layout()
    plt.show()

    # æœ€çŸ­è·¯å¾„æ­¥æ•°åˆ†å¸ƒï¼ˆç›´æ–¹å›¾ï¼‰
    plt.figure(figsize=(8, 4))
    plt.hist(all_min_steps, bins=range(min(all_min_steps), max(all_min_steps)+1), edgecolor='black')
    plt.title("distribution_of_spath_step")
    plt.xlabel("shortest_path_step")
    plt.ylabel("appear_time")
    plt.grid(True, axis='y')
    plt.tight_layout()
    plt.show()

    # å¯é€‰ï¼šæ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print("å¹³å‡è®­ç»ƒè½®æ•°ï¼š", np.mean(all_total_episodes))
    print("å¹³å‡é¦–æ¬¡æˆåŠŸè½®æ•°ï¼š", np.mean(all_first_success))
    print("å¹³å‡æœ€çŸ­è·¯å¾„å‘ç°è½®æ•°ï¼š", np.mean(all_shortest_found))
    print("å¹³å‡æœ€çŸ­è·¯å¾„æ­¥æ•°ï¼š", np.mean(all_min_steps))