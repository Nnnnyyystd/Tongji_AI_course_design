import matplotlib.pyplot as plt
import numpy as np
import random
from environment import Env
from collections import defaultdict
from visual import plot_rewards, plot_q_value_heatmap, visualize_path_from_states

class QLearningAgent:
    def __init__(self, actions):
        # actions = [0, 1, 2, 3]
        self.actions = actions
        self.learning_rate = 0.1
        self.discount_factor = 0.9
        #self.epsilon = 0.1
        self.epsilon =0.1  # åˆå§‹ Îµï¼ˆé¼“åŠ±æ¢ç´¢ï¼‰
        self.epsilon_min = 0.05  # æœ€å° Îµï¼ˆè®­ç»ƒåæœŸä»ä¿ç•™å°‘é‡æ¢ç´¢ï¼‰
        self.epsilon_decay = 0.995  # è¡°å‡ç³»æ•°ï¼ˆæˆ–ç”¨çº¿æ€§è¡°å‡ä¹Ÿå¯ä»¥ï¼‰

        self.q_table = defaultdict(lambda: [0.0, 0.0, 0.0, 0.0])

    # é‡‡æ · <s, a, r, s'>
    def learn(self, state, action, reward, next_state):
        current_q = self.q_table[state][action]
        # è´å°”æ›¼æ–¹ç¨‹æ›´æ–°
        new_q = reward + self.discount_factor * max(self.q_table[next_state])
        self.q_table[state][action] += self.learning_rate * (new_q - current_q)

    # ä»Q-tableä¸­é€‰å–åŠ¨ä½œ
    def get_action(self, state):
        if np.random.rand() < self.epsilon:
            # è´ªå©ªç­–ç•¥éšæœºæ¢ç´¢åŠ¨ä½œ
            action = np.random.choice(self.actions)
        else:
            # ä»qè¡¨ä¸­é€‰æ‹©
            state_action = self.q_table[state]
            action = self.arg_max(state_action)
        return action

    @staticmethod
    def arg_max(state_action):
        max_index_list = []
        max_value = state_action[0]
        for index, value in enumerate(state_action):
            if value > max_value:
                max_index_list.clear()
                max_value = value
                max_index_list.append(index)
            elif value == max_value:
                max_index_list.append(index)
        return random.choice(max_index_list)

if __name__ == "__main__":
    env = Env()
    agent = QLearningAgent(actions=list(range(env.n_actions)))

    rewards = []
    min_success_steps = float('inf')
    tolerance = 2
    pause_after_better_path = 100
    pruning_paused_until = -1
    skip_count = 0
    skip_count_threshold = 50
    max_episodes = 2000

    first_success_episode = None  # âœ… é¦–æ¬¡æˆåŠŸè·¯å¾„å‡ºç°æ—¶é—´
    best_path = []  # âœ… æœ€çŸ­è·¯å¾„çŠ¶æ€åºåˆ—ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰
    cnt=0
    for episode in range(max_episodes):
        state = env.reset()
        step_count = 0
        total_reward = 0
        success = False
        early_terminated = False
        episode_path = []

        # âœ… æ˜¯å¦å¯ç”¨å‰ªæ
        pruning_enabled = (
                first_success_episode is not None and
                episode >= pruning_paused_until
        )

        while True:
            env.render()
            action = agent.get_action(str(state))
            episode_path.append(state)  # âœ… è·¯å¾„è®°å½•
            next_state, reward, done = env.step(action)
            step_count += 1
            total_reward += reward

            # âœ… å‰ªæï¼šåªåœ¨æˆåŠŸåå‰ª + æœªæš‚åœæ—¶å¯ç”¨
            #if pruning_enabled and reward > 0 and step_count > min_success_steps + tolerance:
             #   print(f"[{episode}] âš ï¸ å‰ªæï¼šè·¯å¾„ {step_count} æ­¥ > å½“å‰æœ€ä¼˜ {min_success_steps} æ­¥")
              #  early_terminated = True
               # skip_count += 1
               # break

            agent.learn(str(state), action, reward, str(next_state))
            state = next_state

            if done:
                if reward > 0 and not early_terminated:
                    cnt+=1
                    success = True

                    # âœ… ç¬¬ä¸€æ¬¡æˆåŠŸï¼Œåˆå§‹åŒ–å‰ªæå¯åŠ¨æ—¶é—´
                    if first_success_episode is None:
                        first_success_episode = episode
                        pruning_paused_until = episode + 50  # ç­‰ 50 å›åˆå†å‰ª

                    if step_count < min_success_steps:
                        min_success_steps = step_count
                        skip_count = 0
                        pruning_paused_until = episode + pause_after_better_path
                        best_path = episode_path.copy()
                        print(f"[{episode}] ğŸ¯ å‘ç°æ›´çŸ­è·¯å¾„ï¼š{step_count} æ­¥ â†’ æš‚åœå‰ªæè‡³ Episode {pruning_paused_until}")
                break

        env.print_value_all(agent.q_table)
        rewards.append(total_reward)
        # âœ… Îµ è¡°å‡
       # if agent.epsilon > agent.epsilon_min:
        #        agent.epsilon *= agent.epsilon_decay

        # âœ… è¾¾åˆ°å‰ªæå‘½ä¸­æ¬¡æ•° â†’ åˆ¤å®šæ”¶æ•›
        if skip_count >= skip_count_threshold:
            print("ğŸ è®­ç»ƒå®Œæˆï¼æ¨æµ‹æœ€çŸ­è·¯å¾„ä¸ºï¼š", min_success_steps)
            break

        if episode % 10 == 0:
            print(f"[Episode {episode}] Steps: {step_count}, Reward: {total_reward}, Success: {success}")
            print(f"[Episode {episode}] å½“å‰ Îµ = {agent.epsilon:.4f}")


    plot_q_value_heatmap(env, agent.q_table)
    plot_rewards(rewards)
    if 'best_path' in locals() and len(best_path) > 0:
        print("ğŸ¯ æœ€ç»ˆæœ€çŸ­è·¯å¾„æ­¥æ•°ï¼š", len(best_path))
        visualize_path_from_states(env, best_path)
    else:
        print("âš ï¸ æœªè®°å½•åˆ°æˆåŠŸè·¯å¾„ï¼Œæ— æ³•å¯è§†åŒ–")

    env.mainloop()

