
import os
import csv
import torch
from PIL import Image
from tqdm import tqdm
from transformers import CLIPProcessor, CLIPModel

# ====== é…ç½®è·¯å¾„ ======
#image_folder = "./generated_full_finetuned"  # å›¾åƒç›®å½•
image_folder = "./generated_frozen_finetuned"  # å›¾åƒç›®å½•
csv_path = "./ss.csv"  # åŒ…å«è¡¨å¤´çš„ CSV æ–‡ä»¶

# ====== è¯»å– CSV æ–‡ä»¶ï¼ˆå¸¦è¡¨å¤´ï¼‰======
captions = {}
with open(csv_path, "r", encoding="utf-8-sig") as f:
    reader = csv.DictReader(f)
    for row in reader:
        fname = row["file_name"].strip() + ".png"  # âœ… åŠ ä¸Š .png
        prompt = row["text"].strip()
        captions[fname] = prompt
print(f"ğŸ“‹ æç¤ºè¯æ€»æ•°: {len(captions)}")
print("ğŸ“Œ ç¤ºä¾‹ï¼š", list(captions.items())[:3])

# ====== åŠ è½½ CLIP æ¨¡å‹ ======
device = "cuda" if torch.cuda.is_available() else "cpu"
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(device)
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# ====== éå†å›¾åƒå¹¶è®¡ç®— CLIPScore ======
scores = []
matched = 0

for filename in tqdm(os.listdir(image_folder)):
    if filename.lower().endswith((".png", ".jpg", ".jpeg")):
        prompt = captions.get(filename)
        if not prompt:
            print(f"[!] æ²¡æ‰¾åˆ°å¯¹åº” prompt: {filename}")
            continue

        matched += 1
        image_path = os.path.join(image_folder, filename)
        image = Image.open(image_path).convert("RGB")
        inputs = processor(text=[prompt], images=image, return_tensors="pt", padding=True).to(device)

        with torch.no_grad():
            outputs = model(**inputs)
            score = outputs.logits_per_image.item()
            scores.append(score)

# ====== è¾“å‡ºç»“æœ ======
if scores:
    avg_score = sum(scores) / len(scores)
    print(f"\nâœ… å¹³å‡ CLIPScore: {avg_score:.4f}ï¼Œå…±è®¡å›¾åƒæ•°: {len(scores)}ï¼ŒæˆåŠŸåŒ¹é…: {matched}")
else:
    print("âŒ æ²¡æœ‰æˆåŠŸè®¡ç®— CLIPScore")

