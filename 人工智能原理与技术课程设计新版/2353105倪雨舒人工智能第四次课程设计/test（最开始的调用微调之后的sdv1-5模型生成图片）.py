


import torch
from pathlib import Path
from diffusers import StableDiffusionPipeline, UNet2DConditionModel, AutoencoderKL, DDIMScheduler
from transformers import CLIPTextModel, CLIPTokenizer

# âœ… è·¯å¾„è®¾ç½®
model_path = "./Chinese_painting_model_full_ema"
checkpoint_unet = f"{model_path}/checkpoint-20000/unet"
base_model = "./stable-diffusion-v1-5"  # ğŸŸ¡ scheduler æ”¹ä»è¿™é‡ŒåŠ è½½ï¼

# âœ… é€æ¨¡å—åŠ è½½
unet = UNet2DConditionModel.from_pretrained(checkpoint_unet, torch_dtype=torch.float16)
vae = AutoencoderKL.from_pretrained(f"{model_path}/vae", torch_dtype=torch.float16)
text_encoder = CLIPTextModel.from_pretrained(f"{model_path}/text_encoder", torch_dtype=torch.float16)
tokenizer = CLIPTokenizer.from_pretrained(f"{model_path}/tokenizer")
scheduler = DDIMScheduler.from_pretrained(f"{base_model}/scheduler")  # âœ… ä» base æ¨¡å‹åŠ è½½ scheduler

# âœ… æ„å»º pipeline
pipe = StableDiffusionPipeline(
    unet=unet,
    vae=vae,
    text_encoder=text_encoder,
    tokenizer=tokenizer,
    scheduler=scheduler,
    safety_checker=None,
    feature_extractor=None,
).to("cuda")

# âœ… æµ‹è¯•ç”Ÿæˆ
prompt = "ç™½æ—¥ä¾å±±å°½ï¼Œé»„æ²³å…¥æµ·æµã€‚"
output_dir = Path("test_outputs_fixed_scheduler")
output_dir.mkdir(exist_ok=True)

for i in range(20):
    with torch.autocast("cuda"):
        image = pipe(prompt, guidance_scale=7.5, num_inference_steps=30).images[0]
    image.save(output_dir / f"sample_fixed_{i+1}.png")
    print(f"âœ… Saved sample_fixed_{i+1}.png")





























# from diffusers import StableDiffusionPipeline, UNet2DConditionModel, AutoencoderKL
# from transformers import CLIPTextModel
# import torch

# # åŸå§‹è·¯å¾„
# base_model_path = "./stable-diffusion-v1-5"
# finetuned_model_path = "./Chinese_painting_model_full_ema"
# checkpoint_path = f"{finetuned_model_path}/checkpoint-20000"

# # åŠ è½½ base æ¨¡å‹ç»“æ„
# pipe = StableDiffusionPipeline.from_pretrained(
#     base_model_path,
#     torch_dtype=torch.float16,
#     safety_checker=None
# )

# # æ›¿æ¢ä¸ºå¾®è°ƒåçš„ UNet
# pipe.unet = UNet2DConditionModel.from_pretrained(f"{checkpoint_path}/unet", torch_dtype=torch.float16)

# # æ›¿æ¢ä¸ºå·²ç»ä¿å­˜çš„ Text Encoder å’Œ VAE
# pipe.text_encoder = CLIPTextModel.from_pretrained(f"{finetuned_model_path}/text_encoder", torch_dtype=torch.float16)
# pipe.vae = AutoencoderKL.from_pretrained(f"{finetuned_model_path}/vae", torch_dtype=torch.float16)

# # ä¿å­˜ä¸ºå®Œæ•´ pipelineï¼Œç”Ÿæˆ model_index.json âœ…
# pipe.save_pretrained(finetuned_model_path)
# print("âœ… å·²ç”Ÿæˆ model_index.jsonï¼Œå¹¶å®Œæˆå®Œæ•´æ¨¡å‹ä¿å­˜")
