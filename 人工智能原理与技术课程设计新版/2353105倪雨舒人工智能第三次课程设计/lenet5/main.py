

import os
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
from tqdm import tqdm
from sklearn.metrics import accuracy_score
import mindspore
from mindspore import nn
from mindspore.dataset import vision, transforms, ImageFolderDataset
from mindspore import save_checkpoint, load_checkpoint, load_param_into_net

# 1. æ•°æ®è·¯å¾„é…ç½®
data_dir = "data"
train_dir = os.path.join(data_dir, "train")
test_dir = os.path.join(data_dir, "test")

# 2. æ•°æ®é¢„å¤„ç†å’ŒåŠ è½½ï¼ˆé€šé“ç»´ä¿®å¤ï¼‰
def datapipe(data_dir, batch_size):
    dataset = ImageFolderDataset(data_dir, shuffle=True)

    image_transforms = [
        vision.Decode(),
        vision.ToPIL(),
        vision.Resize((28, 28)),
        vision.Grayscale(1),
        vision.Rescale(1.0 / 255.0, 0),
        vision.Normalize(mean=(0.1307,), std=(0.3081,)),
        vision.HWC2CHW(),
        lambda x: x.reshape(1, 28, 28)  # ğŸ‘ˆ ä¿®å¤é€šé“ç»´ï¼Œç¡®ä¿ shape ä¸º (1, 28, 28)
    ]
    label_transform = transforms.TypeCast(mindspore.int32)

    dataset = dataset.map(image_transforms, input_columns="image")
    dataset = dataset.map(label_transform, input_columns="label")
    dataset = dataset.batch(batch_size)
    return dataset

train_dataset = datapipe(train_dir, 64)
test_dataset = datapipe(test_dir, 64)

#3. æ¨¡å‹å®šä¹‰ï¼ˆLeNet5ï¼‰
# class LeNet5(nn.Cell):
#     def __init__(self, num_classes=26):
#         super(LeNet5, self).__init__()
#         self.conv1 = nn.Conv2d(1, 6, kernel_size=5, pad_mode='valid')
#         self.relu1 = nn.ReLU()
#         self.pool1 = nn.AvgPool2d(kernel_size=(2, 2), stride=(2, 2))
#         self.conv2 = nn.Conv2d(6, 16, kernel_size=5, pad_mode='valid')
#         self.relu2 = nn.ReLU()
#         self.pool2 = nn.AvgPool2d(kernel_size=(2, 2), stride=(2, 2))
#         self.flatten = nn.Flatten()
#         self.fc1 = nn.Dense(16 * 4 * 4, 120)
#         self.fc2 = nn.Dense(120, 84)
#         self.fc3 = nn.Dense(84, num_classes)
#
#     def construct(self, x):
#         x = self.conv1(x)
#         x = self.relu1(x)
#         x = self.pool1(x)
#         x = self.conv2(x)
#         x = self.relu2(x)
#         x = self.pool2(x)
#         x = self.flatten(x)
#         x = self.fc1(x)
#         x = self.fc2(x)
#         x = self.fc3(x)
#         return x
class LeNet5(nn.Cell):
    def __init__(self, num_classes=26):
        super(LeNet5, self).__init__()
        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, pad_mode='valid')
        self.bn1 = nn.BatchNorm2d(6)
        self.relu1 = nn.ReLU()
        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)

        self.conv2 = nn.Conv2d(6, 16, kernel_size=5, pad_mode='valid')
        self.bn2 = nn.BatchNorm2d(16)
        self.relu2 = nn.ReLU()
        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)

        self.flatten = nn.Flatten()
        self.fc1 = nn.Dense(16 * 4 * 4, 84)  # âœ… é™ä½ç»´åº¦
        self.dropout1 = nn.Dropout(keep_prob=0.5)
        self.fc2 = nn.Dense(84, num_classes)

    def construct(self, x):
        x = self.pool1(self.relu1(self.bn1(self.conv1(x))))
        x = self.pool2(self.relu2(self.bn2(self.conv2(x))))
        x = self.flatten(x)
        x = self.dropout1(self.fc1(x))
        x = self.fc2(x)
        return x
model = LeNet5(num_classes=26)

# 4. æŸå¤±å‡½æ•°ä¸ä¼˜åŒ–å™¨
loss_fn = nn.CrossEntropyLoss()
optimizer = nn.SGD(model.trainable_params(), 1e-2)

# 5. å‰å‘ + åå‘å‡½æ•°å®šä¹‰
def forward_fn(data, label):
    logits = model(data)
    loss = loss_fn(logits, label)
    return loss, logits

grad_fn = mindspore.value_and_grad(forward_fn, None, optimizer.parameters, has_aux=True)

def train_step(data, label):
    (loss, _), grads = grad_fn(data, label)
    optimizer(grads)
    return loss

# 6. è®­ç»ƒå‡½æ•°
def train(model, dataset):
    size = dataset.get_dataset_size()           # è·å–è®­ç»ƒé›†ä¸­ batch æ€»æ•°ï¼ˆç”¨äº tqdm æ˜¾ç¤ºï¼‰
    model.set_train()                           # è®¾ç½®æ¨¡å‹ä¸ºè®­ç»ƒæ¨¡å¼ï¼ˆå¯ç”¨ Dropout/BN ç­‰ï¼‰
    correct = 0                                  # ç»Ÿè®¡é¢„æµ‹æ­£ç¡®çš„æ ·æœ¬æ•°
    total = 0                                    # æ ·æœ¬æ€»æ•°
    total_loss = 0                               # ç´¯åŠ  loss
    for batch, (data, label) in enumerate(tqdm(dataset.create_tuple_iterator(), total=size, desc="Training")):
        loss = train_step(data, label)           # æ‰§è¡Œä¸€æ¬¡ train_stepï¼ˆå‰å‘+åå‘+æ›´æ–°ï¼‰
        total_loss += loss.asnumpy()             # ç´¯åŠ æŸå¤±ï¼ˆè½¬ä¸º numpy æ ‡é‡ï¼‰
        logits = model(data)                     # å†å‰å‘ä¸€æ¬¡ç”¨äºè¯„ä¼°å‡†ç¡®ç‡
        correct += (logits.argmax(1) == label).asnumpy().sum()  # argmax å–é¢„æµ‹ç±»åˆ«ï¼Œä¸æ ‡ç­¾å¯¹æ¯”
        total += label.shape[0]                  # æ ·æœ¬æ€»æ•°
        if batch % 10 == 0:                      # æ¯ 10 ä¸ª batch è¾“å‡ºä¸€æ¬¡ä¸­é—´ loss
            print(f"loss: {loss.asnumpy():>7f}  [{batch:>3d}/{size:>3d}]")

    avg_loss = total_loss / size                 # å¹³å‡æ¯ batch çš„æŸå¤±
    accuracy = correct / total                   # æ€»ä½“å‡†ç¡®ç‡
    print(f"Train: Accuracy: {(100*accuracy):>0.1f}%, Avg loss: {avg_loss:>8f}\n")
    return avg_loss, accuracy                    # è¿”å›ç»™ä¸»å¾ªç¯è®°å½•

# 7. æµ‹è¯•å‡½æ•°
def test(model, dataset, loss_fn):
    num_batches = dataset.get_dataset_size()
    model.set_train(False)
    total, test_loss, correct = 0, 0, 0
    all_preds = []
    all_labels = []

    for data, label in dataset.create_tuple_iterator():
        pred = model(data)
        loss = loss_fn(pred, label)
        test_loss += loss.asnumpy()
        correct += (pred.argmax(1) == label).asnumpy().sum()
        total += label.shape[0]
        all_preds.extend(pred.argmax(1).asnumpy())
        all_labels.extend(label.asnumpy())

    test_loss /= num_batches
    accuracy = correct / total
    conf_matrix = confusion_matrix(all_labels, all_preds)
    print(f"Test: Accuracy: {(100*accuracy):>0.1f}%, Avg loss: {test_loss:>8f}\n")
    return test_loss, accuracy, conf_matrix

#8. ä¸»è®­ç»ƒå¾ªç¯ï¼ˆå« EarlyStoppingï¼‰
train_losses = []
train_accuracies = []
test_losses = []
test_accuracies = []
final_conf_matrix = None

best_loss = float('inf')
early_stop_counter = 0
patience = 100  # è¿ç»­å¤šå°‘è½®éªŒè¯é›† loss æœªä¸‹é™å°±æå‰ç»ˆæ­¢

for t in range(1000):  # æœ€å¤šè®­ç»ƒ 1000 è½®
    print(f"Epoch {t+1}\n-------------------------------")
    train_loss, train_acc = train(model, train_dataset)
    test_loss, test_acc, conf_matrix = test(model, test_dataset, loss_fn)

    train_losses.append(train_loss)
    train_accuracies.append(train_acc)
    test_losses.append(test_loss)
    test_accuracies.append(test_acc)
    final_conf_matrix = conf_matrix

    # EarlyStopping åˆ¤æ–­ä¸æ¨¡å‹ä¿å­˜
    if test_loss < best_loss:
        best_loss = test_loss
        early_stop_counter = 0
        save_checkpoint(model, "best_model.ckpt")
        print(" Best model updated.")
    else:
        early_stop_counter += 1
        print(f" No improvement in test loss for {early_stop_counter} epoch(s).")
        if early_stop_counter >= patience:
            print(f" Early stopping triggered at epoch {t+1}")
            break

#9. ä¿å­˜æ··æ·†çŸ©é˜µå’Œæ¨¡å‹
np.save("final_confusion_matrix.npy", final_conf_matrix)
with open("final_confusion_matrix.txt", "w") as f:
    f.write("Final Confusion Matrix:\n")
    for row in final_conf_matrix:
        f.write(" ".join(map(str, row)) + "\n")
print("æ··æ·†çŸ©é˜µå·²ä¿å­˜")

save_checkpoint(model, "model.ckpt")
print("æ¨¡å‹å·²ä¿å­˜åˆ° model.ckpt")

# 10. ä¿å­˜è®­ç»ƒæ›²çº¿
np.save("train_losses.npy", np.array(train_losses))
np.save("train_accuracies.npy", np.array(train_accuracies))
np.save("test_losses.npy", np.array(test_losses))
np.save("test_accuracies.npy", np.array(test_accuracies))

# 11. æ¨¡å‹åŠ è½½æµ‹è¯•

model = LeNet5(num_classes=26)
param_dict = load_checkpoint("best_model.ckpt")
load_param_into_net(model, param_dict)
model.set_train(False)

# æ”¶é›†æ‰€æœ‰é¢„æµ‹ä¸çœŸå®æ ‡ç­¾
all_preds = []
all_labels = []

for data, label in test_dataset.create_tuple_iterator():
    pred = model(data)
    predicted = pred.argmax(1)
    all_preds.extend(predicted.asnumpy())
    all_labels.extend(label.asnumpy())

# è®¡ç®—å‡†ç¡®ç‡
accuracy = accuracy_score(all_labels, all_preds)
print(f"âœ… æœ€ä½³æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„å‡†ç¡®ç‡ä¸ºï¼š{accuracy * 100:.2f}%")